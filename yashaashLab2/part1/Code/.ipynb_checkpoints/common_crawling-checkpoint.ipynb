{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Crawl Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/Smerity/56bc6f21a8adec920ebf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import requests\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import urllib.request\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter domain to searchmarvel.com\n"
     ]
    }
   ],
   "source": [
    "#user enters the domain to search\n",
    "domain_to_search=input('enter domain to search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter month of 2019 to search in format MM03\n"
     ]
    }
   ],
   "source": [
    "#user enters month to search data\n",
    "month_to_search=input('enter month of 2019 to search in format MM')\n",
    "if month_to_search=='01':\n",
    "    month_offset='04'\n",
    "elif month_to_search=='02':\n",
    "    month_offset='09'\n",
    "elif month_to_search=='03':\n",
    "    month_offset='13'\n",
    "else:\n",
    "    print('month data unavailable till now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert domain name to character list\n",
    "#replace special characters : and / with their url equivalents\n",
    "#save in a new list then combine list to form string to feed url\n",
    "domain_to_list=list(domain_to_search)\n",
    "domain_final=list()\n",
    "for i in domain_to_list:\n",
    "    if i==':':\n",
    "        domain_final.append('%3A')\n",
    "    elif i=='/':\n",
    "        domain_final.append('%2F')\n",
    "    else:\n",
    "        domain_final.append(i)\n",
    "final_domain=''.join(domain_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search common_crawl index\n",
    "response_json= requests.get('https://index.commoncrawl.org/CC-MAIN-2019-'+month_offset+'-index?url='+final_domain+'&output=json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the json responses into pages\n",
    "#most pages are copied by common crawl only once per month, so we pick only the first page\n",
    "pages = [json.loads(x) for x in response_json.text.strip().split('\\n')]\n",
    "page=pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset, length = int(page['offset']), int(page['length'])\n",
    "offset_end = offset + length - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if page length<1000, it is mostly 301(page moved permanently) or 404(page not found) data\n",
    "#it is better to stop here and look for another domain since this will not yield any data\n",
    "if int(page['length'])>1000:\n",
    "    prefix = \"https://commoncrawl.s3.amazonaws.com/\"\n",
    "    request=urllib.request.Request(prefix + page['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
    "    response=urllib.request.urlopen(request)\n",
    "    unzipped_file=gzip.GzipFile(fileobj=response)\n",
    "    before_soup=unzipped_file.read()\n",
    "else:\n",
    "    print(\"try another domain\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create soup\n",
    "soup=BeautifulSoup(before_soup,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract para tags the page and store in a txt file of domain name\n",
    "#this p needs to be changed at times to article since a lot of sites use article instead of p for para content now\n",
    "f1=open(domain_to_search+'.txt','a')\n",
    "a_tags=soup.find_all('a')\n",
    "for tag in a_tags:\n",
    "    p_tags=soup.find_all('p') \n",
    "    for p_tag in p_tags:\n",
    "        f1.write(str(p_tag))\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
